{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from utils.load import download_flickr, download_glove, load_data, load_split_dataset\n",
    "from utils.explore import (\n",
    "    get_descriptive_statistics,\n",
    "    explore_dataset,\n",
    "    visualize_image,\n",
    "    get_captions,\n",
    ")\n",
    "from utils.split import split_and_save_data\n",
    "from utils.preprocessing import get_vocabulary, resize_image_dictionary, pad_images\n",
    "\n",
    "from keras.applications.inception_v3 import (\n",
    "    InceptionV3,\n",
    "    preprocess_input,\n",
    "    decode_predictions,\n",
    ")\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Flatten, Input, Dropout, Dense, Embedding, LSTM, MaxPooling2D, Conv2D, add\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"data/Images\"\n",
    "image_folder_homogeneous = \"data/Images_homogeneous\"\n",
    "captions_file = \"data/captions.txt\"\n",
    "split_data = \"data/split_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Downloading the Dataset\n",
    "\n",
    "If not already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/Image\"):\n",
    "    download_flickr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Exploring the Dataset\n",
    "\n",
    "Valid if the dataset has already been loaded into your `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_descriptive_statistics(image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_dataset(image_folder, captions_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding padding so that all images have the same dimensions\n",
    "if not os.path.exists(image_folder_homogeneous):\n",
    "    pad_images(image_folder, image_folder_homogeneous, target_size=(500, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arrays, image_captions = load_data(image_folder_homogeneous, captions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(image_arrays), type(image_captions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Captions\n",
    "\n",
    "We exploring the `data/captions.txt` file, we notice that the captions seem to end with a `.`, so let's just remove this to have cleaner captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_captions = {\n",
    "    img_name: [cap.replace('.', '').strip().lower() for cap in captions]\n",
    "    for img_name, captions in image_captions.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Vocabulary\n",
    "\n",
    "The vocabulary contains all the unique words present in the captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = get_vocabulary(image_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data, train_captions, val_captions, test_captions = split_and_save_data(image_arrays, image_captions, save_folderpath=split_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 129\n",
    "visualize_image(train_data, i)\n",
    "print(\n",
    "    get_captions(train_captions, i)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Extracting a Feature Vector from each image\n",
    "\n",
    "We can extract the main features of an image with a pre-trained model, which has been trained in a large dataset. This way we can get from each image a vector containing the main characteristics of this image. We will use the **InceptionV3** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ith_value(dict, i):\n",
    "    return list(dict.values())[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = InceptionV3(weights=\"imagenet\")\n",
    "model = Model(base_model.input, base_model.layers[-2].output)\n",
    "\n",
    "\n",
    "def encode_images(\n",
    "    img_dict: dict, model: Model = model, batch_size: int = 32\n",
    ") -> dict[str, np.ndarray]:\n",
    "    print(\"resizing the images...\")\n",
    "    resized_images = resize_image_dictionary(img_dict, (299, 299))\n",
    "    feature_vectors = {}\n",
    "    print(\"extracting the feature vectors...\")\n",
    "    img_names = list(resized_images.keys())\n",
    "    img_arrays = list(resized_images.values())\n",
    "    # preprocessing all images\n",
    "    img_arrays = [preprocess_input(img_array) for img_array in img_arrays]\n",
    "\n",
    "    for start_index in range(0, len(img_names), batch_size):\n",
    "        end_index = min(start_index + batch_size, len(img_names))\n",
    "        batch_images = img_arrays[start_index:end_index]\n",
    "        batch_names = img_names[start_index:end_index]\n",
    "        # predicting the feature vectors for the batch\n",
    "        batch_vectors = model.predict(np.array(batch_images))\n",
    "        # reshaping and storing the feature vectors\n",
    "        for i, img_name in enumerate(batch_names):\n",
    "            vec = np.reshape(batch_vectors[i], (batch_vectors.shape[1]))\n",
    "            feature_vectors[img_name] = vec\n",
    "        \n",
    "    return feature_vectors\n",
    "\n",
    "\n",
    "fea_vec_train = encode_images(train_data)\n",
    "\n",
    "print(fea_vec_train)\n",
    "print(len(list(train_data.values())), len(fea_vec_train.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Tokenization of the Vocabulary\n",
    "\n",
    "We need to tokenize our vocabulary to make it understandable for a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_apparitions = 10  # defining a threshold of number of apparitions in the text.\n",
    "\n",
    "text = \" \".join(\n",
    "    [\" \".join(train_captions[img_name]) for img_name in list(train_captions.keys())]\n",
    ")\n",
    "# doit-on mettre les 3 dict captions ou non ?\n",
    "\n",
    "words = text.split(\" \")\n",
    "\n",
    "# this returns a dictionary containing the words and the number of their occurrences in the text\n",
    "words_count = Counter(words)\n",
    "\n",
    "# keeping just the important words\n",
    "important_words = [\n",
    "    word for word, n_occ in words_count.items() if n_occ >= min_apparitions\n",
    "]\n",
    "\n",
    "# mapping the words to integers\n",
    "int_to_word = {i: word for i, word in enumerate(important_words)}\n",
    "word_to_int = {word: i for i, word in enumerate(important_words)}\n",
    "\n",
    "vocabulary_size = len(list(int_to_word.keys()))\n",
    "print(vocabulary_size)\n",
    "\n",
    "# find the maximum length of a description in a dataset\n",
    "max_length = max(\n",
    "    [\n",
    "        max([len(caption.split(\" \")) for caption in captions])\n",
    "        for captions in list(train_captions.values())\n",
    "    ]\n",
    ")\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/glove\"):\n",
    "    download_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizing for computation\n",
    "target_size = (128, 128)\n",
    "train_data = resize_image_dictionary(train_data, target_size)\n",
    "val_data = resize_image_dictionary(val_data, target_size)\n",
    "test_data = resize_image_dictionary(test_data, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, X2, y = [], [], []\n",
    "\n",
    "for img_name, caps in train_captions.items():\n",
    "    img_array = train_data[img_name]\n",
    "\n",
    "    for cap in caps:\n",
    "        sequence = [word_to_int[word] for word in cap.split(\" \") if word in word_to_int]\n",
    "\n",
    "        for i in range(1, len(sequence)):\n",
    "            input_sequence = sequence[:i]\n",
    "            output_sequence = sequence[i]\n",
    "\n",
    "            input_sequence = pad_sequences([input_sequence], maxlen=max_length)[0]\n",
    "            output_sequence = to_categorical(\n",
    "                [output_sequence], num_classes=vocabulary_size\n",
    "            )[0]\n",
    "\n",
    "            X1.append(img_array)\n",
    "            X2.append(input_sequence)\n",
    "            y.append(output_sequence)\n",
    "\n",
    "X2 = np.array(X2)\n",
    "X1 = np.array(X1)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "# load glove vectors for embedding layer\n",
    "embeddings_index = {}\n",
    "glove_path = \"data/glove/glove.6B.200d.txt\"\n",
    "glove = open(glove_path, \"r\", encoding=\"utf-8\").read()\n",
    "for line in glove.split(\"\\n\"):\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    if word != '':\n",
    "        indices = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
    "print(embedding_matrix.shape)\n",
    "for word, i in word_to_int.items():\n",
    "    if word != '' and word in embeddings_index:\n",
    "        embedding_vector = embeddings_index[word]\n",
    "        if embedding_vector is not None and embedding_vector.shape != (0,):\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = Input(shape=(target_size[0], target_size[1], 3))\n",
    "\n",
    "# Add convolutional layers for feature extraction\n",
    "conv_1 = Conv2D(64, (3, 3), activation='relu')(input_1)\n",
    "pool_1 = MaxPooling2D((2, 2))(conv_1)\n",
    "conv_2 = Conv2D(128, (3, 3), activation='relu')(pool_1)\n",
    "pool_2 = MaxPooling2D((2, 2))(conv_2)\n",
    "conv_3 = Conv2D(256, (3, 3), activation='relu')(pool_2)\n",
    "pool_3 = MaxPooling2D((2, 2))(conv_3)\n",
    "flattened_features = Flatten()(pool_3)\n",
    "\n",
    "# features_1 = Dropout(0.2)(input_1)\n",
    "features_1 = Dropout(0.2)(flattened_features)\n",
    "features_2 = Dense(256, activation=\"relu\")(features_1)\n",
    "\n",
    "input_2 = Input(shape=(max_length,))\n",
    "sequence_1 = Embedding(vocabulary_size, embedding_dim, mask_zero=True)(input_2)\n",
    "sequence_2 = Dropout(0.2)(sequence_1)\n",
    "sequence_3 = LSTM(256)(sequence_2)\n",
    "\n",
    "decoder_1 = add([features_2, sequence_3])\n",
    "decoder_2 = Dense(256, activation=\"relu\")(decoder_1)\n",
    "\n",
    "flattened_output = Flatten()(decoder_2)\n",
    "outputs = Dense(vocabulary_size, activation=\"softmax\")(flattened_output)\n",
    "model = Model(inputs=[input_1, input_2], outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    if isinstance(layer, Embedding):\n",
    "        layer.set_weights([embedding_matrix])\n",
    "        layer.trainable = False\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "model.fit([X1, X2], y, epochs=50, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Predicting the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_array: np.ndarray) -> str:\n",
    "    start = \"startseq\"\n",
    "    for i in range(max_length):\n",
    "        sequence = [word_to_int[word] for word in start.split() if word in word_to_int]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "\n",
    "        y_pred = model.predict([image_array, sequence])\n",
    "        y_pred = np.argmax(y_pred)\n",
    "\n",
    "        word = int_to_word[y_pred]\n",
    "        start += ' ' + word\n",
    "        if word == \"endseq\":\n",
    "            break\n",
    "\n",
    "    final = start.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final\n",
    "\n",
    "print(\"please test the model!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
